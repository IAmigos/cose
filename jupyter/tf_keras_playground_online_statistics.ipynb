{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"tf_keras_playground_online_statistics.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"Xm2YGKcYn8Fz","colab_type":"code","colab":{}},"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import time\n","\n","tf.enable_eager_execution()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"46CKgHnEoGnN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":269},"outputId":"8b760a60-4913-4856-eb78-fa4aded7aa51","executionInfo":{"status":"ok","timestamp":1550247973790,"user_tz":-60,"elapsed":504,"user":{"displayName":"Emre Aksan","photoUrl":"","userId":"07313602013607958558"}}},"cell_type":"code","source":["# For sequences\n","\n","def online_statistics_sequences(data):\n","  n_samples = 0\n","  n_all, mean_all, var_all, m2_all = 0, 0, 0, 0\n","  n_channel, mean_channel, var_channel, m2_channel = 0, 0, 0, 0\n","  min_all, max_all = np.inf, -np.inf\n","  min_seq_len, max_seq_len = np.inf, -np.inf\n","\n","  for x in data: \n","    n_samples += 1\n","    seq_len, feature_size = x.shape\n","\n","    # Global mean&var\n","    n_all += seq_len*feature_size\n","    delta_all = x - mean_all\n","    mean_all = mean_all + delta_all.sum()/n_all\n","    m2_all = m2_all + (delta_all*(x - mean_all)).sum()\n","\n","    # Channelwise mean&var\n","    n_channel += seq_len\n","    delta_channel = x - mean_channel\n","    mean_channel = mean_channel + delta_channel.sum(axis=0)/n_channel\n","    m2_channel = m2_channel + (delta_channel*(x - mean_channel)).sum(axis=0)\n","\n","    # Global min&max values.\n","    min_all = np.min(x) if np.min(x) < min_all else min_all\n","    max_all = np.max(x) if np.max(x) > max_all else max_all\n","    \n","    # Min&max sequence length.\n","    min_seq_len = seq_len if seq_len < min_seq_len else min_seq_len\n","    max_seq_len = seq_len if seq_len > max_seq_len else max_seq_len\n","\n","  var_all = m2_all/(n_all-1)\n","  var_channel = m2_channel/(n_channel-1)\n","  stats = dict(mean_all=mean_all, var_all=var_all, mean_channel=mean_channel, var_channel=var_channel, min_all=min_all, max_all=max_all, min_seq_len=min_seq_len, max_seq_len=max_seq_len, n_samples=n_samples)\n","  return stats\n","\n","def offline_statistics_sequences(data):\n","  \"\"\"\n","  data can be list of data samples (in variable length) or a numpy tensor with\n","  samples listed in the first axis.\n","  \"\"\"\n","  all_samples = np.vstack(data)\n","\n","  mean_all = all_samples.mean()\n","  var_all = all_samples.var(ddof=1)\n","  mean_channel = all_samples.mean(axis=0)\n","  var_channel = all_samples.var(axis=0, ddof=1)\n","\n","  min_all = all_samples.min()\n","  max_all = all_samples.max()\n","\n","  seq_lens = np.array([x.shape[0] for x in data])\n","  min_seq_len = seq_lens.min()\n","  max_seq_len = seq_lens.max()\n","  n_samples = len(data)\n","\n","  stats = dict(mean_all=mean_all, var_all=var_all, mean_channel=mean_channel, var_channel=var_channel, min_all=min_all, max_all=max_all, min_seq_len=min_seq_len, max_seq_len=max_seq_len, n_samples=n_samples)\n","  return stats\n","\n","# Quick Test\n","def log_stats(start_time, stats, tag=\"Online\"):\n","  print(\"--- %s seconds ---\" % (time.time() - start_time))\n","  print(\"[{2}] mean: {0}, std: {1}\".format(stats[\"mean_all\"], stats[\"var_all\"], tag))\n","  print(\"[{2}] mean channel: {0}, std channel: {1}\".format(stats[\"mean_channel\"], stats[\"var_channel\"], tag))\n","  print(\"[{1}] # samples: {0}\".format(stats[\"n_samples\"], tag))\n","  print(\"[{2}] min value: {0}, max value: {1}\".format(stats[\"min_all\"], stats[\"max_all\"], tag))\n","  print(\"[{2}] min length: {0}, max length: {1}\".format(stats[\"min_seq_len\"], stats[\"max_seq_len\"], tag))\n","  print(\"============\")\n","\n","num_samples = 10000\n","seq_len = 100\n","feature_size = 3\n","batch_size = 16\n","num_epochs = 2\n","eval_frequency = 2  # in number of training steps.\n","\n","# samples = np.concatenate([np.random.normal(0, 10, (num_samples, seq_len, 1)), np.random.normal(30, 5, (num_samples, seq_len, 2))], axis=-1)\n","samples = [np.random.normal(0, 10, (192, 3)), np.random.normal(0, 30, (19, 3)), np.random.normal(10, 10, (92, 3))]\n","labels = np.random.randint(0, 10, (num_samples, seq_len, feature_size))\n","\n","start_time = time.time()\n","online_stats = online_statistics_sequences(samples)\n","log_stats(start_time, online_stats, \"Online\")\n","\n","\n","start_time = time.time()\n","offline_stats = offline_statistics_sequences(samples)\n","log_stats(start_time, online_stats, \"Offline\")"],"execution_count":18,"outputs":[{"output_type":"stream","text":["--- 0.00043201446533203125 seconds ---\n","[Online] mean: 1.788093508690388, std: 209.0499481567896\n","[Online] mean channel: [1.91043825 0.9617036  2.49213867], std channel: [197.54878069 239.00104677 190.78693344]\n","[Online] # samples: 3\n","[Online] min value: -106.05878061698726, max value: 71.94902591014984\n","[Online] min length: 19, max length: 192\n","============\n","--- 0.0010149478912353516 seconds ---\n","[Offline] mean: 1.788093508690388, std: 209.0499481567896\n","[Offline] mean channel: [1.91043825 0.9617036  2.49213867], std channel: [197.54878069 239.00104677 190.78693344]\n","[Offline] # samples: 3\n","[Offline] min value: -106.05878061698726, max value: 71.94902591014984\n","[Offline] min length: 19, max length: 192\n","============\n"],"name":"stdout"}]},{"metadata":{"id":"NYMBP3tJItXF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":251},"outputId":"77452e53-e978-4525-9ff8-2c358f967cc3","executionInfo":{"status":"ok","timestamp":1550246054575,"user_tz":-60,"elapsed":8795,"user":{"displayName":"Emre Aksan","photoUrl":"","userId":"07313602013607958558"}}},"cell_type":"code","source":["def tf_online_statistics_sequences(iterable_data, key=None):\n","    \"\"\"\n","    Given a da data iterator, gathers data statistics online. The whole data isn't required to be loaded.\n","    It is eager compatible, and hence it is okay to pass numpy array or python list.\n","    Args:\n","        iterable_data: where each sample is assumed to be a dictionary with <key,value> pairs. \n","        key: data of interest.\n","    Returns:\n","        (dict) or data statistics with mean, var, min, max calculated across the sequences or all values, and\n","        seq_len, number of samples.\n","    \"\"\"\n","    n_samples = 0.0\n","    n_all, mean_all, var_all, m2_all = 0.0, 0.0, 0.0, 0.0\n","    n_channel, mean_channel, var_channel, m2_channel = 0.0, 0.0, 0.0, 0.0\n","    min_all, max_all = np.inf, -np.inf\n","    min_seq_len, max_seq_len = np.inf, -np.inf\n","\n","    for sample_dict in iterable_data:\n","        if key is not None:\n","            sample = sample_dict[key]\n","        else:\n","            sample = sample_dict\n","        n_samples += 1\n","        seq_len, feature_size = tf.to_float(sample.shape)\n","\n","        # Global mean&var\n","        n_all += seq_len * feature_size\n","        delta_all = sample - mean_all\n","        mean_all = mean_all + tf.reduce_sum(delta_all) / n_all\n","        m2_all = m2_all + tf.reduce_sum(delta_all * (sample - mean_all))\n","\n","        # Channel-wise mean&var\n","        n_channel += seq_len\n","        delta_channel = sample - mean_channel\n","        mean_channel = mean_channel + tf.reduce_sum(delta_channel, axis=0) / n_channel\n","        m2_channel = m2_channel + tf.reduce_sum(delta_channel * (sample - mean_channel), axis=0)\n","\n","        # Global min&max values.\n","        min_all = np.min(sample) if np.min(sample) < min_all else min_all\n","        max_all = np.max(sample) if np.max(sample) > max_all else max_all\n","\n","        # Min&max sequence length.\n","        min_seq_len = seq_len if seq_len < min_seq_len else min_seq_len\n","        max_seq_len = seq_len if seq_len > max_seq_len else max_seq_len\n","\n","    var_all = m2_all / (n_all - 1)\n","    var_channel = m2_channel / (n_channel - 1)\n","    stats = dict(mean_all=mean_all, var_all=var_all, mean_channel=mean_channel, var_channel=var_channel,\n","                  min_all=min_all, max_all=max_all, min_seq_len=min_seq_len, max_seq_len=max_seq_len,\n","                  n_samples=n_samples)\n","    return stats\n","\n","training_dataset = tf.data.Dataset.from_tensor_slices({\"dummy_samples\":np.float32(samples), \"labels\": samples})\n","start_time = time.time()\n","stats = tf_online_statistics_sequences(training_dataset, key=\"dummy_samples\")\n","log_stats(start_time, stats, \"TF-Online\")"],"execution_count":13,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From <ipython-input-13-c01710ad444b>:24: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","--- 8.227863788604736 seconds ---\n","[TF-Online] mean: 19.989553451538086, std: 250.1465606689453\n","[TF-Online] mean channel: [-1.8944379e-02  2.9992947e+01  2.9994984e+01], std channel: [99.924255 25.002115 24.99739 ]\n","[TF-Online] # samples: 10000.0\n","[TF-Online] min value: -49.19842529296875, max value: 53.97816848754883\n","[TF-Online] min length: 100.0, max length: 100.0\n","============\n"],"name":"stdout"}]},{"metadata":{"id":"jOPSRPGP0KDI","colab_type":"code","colab":{}},"cell_type":"code","source":["# Get eager tensor, run in numpy.\n","def online_statistics_sequences(data, key):\n","  n_samples = 0\n","  n_all, mean_all, var_all, m2_all = 0, 0, 0, 0\n","  n_channel, mean_channel, var_channel, m2_channel = 0, 0, 0, 0\n","  min_all, max_all = np.inf, -np.inf\n","  min_seq_len, max_seq_len = np.inf, -np.inf\n","\n","  for x in data: \n","    if key is not None:\n","      x = x[key]\n","    if isinstance(x, tf.Tensor):\n","      x = x.numpy()\n","\n","    n_samples += 1\n","    seq_len, feature_size = x.shape\n","\n","    # Global mean&var\n","    n_all += seq_len*feature_size\n","    delta_all = x - mean_all\n","    mean_all = mean_all + delta_all.sum()/n_all\n","    m2_all = m2_all + (delta_all*(x - mean_all)).sum()\n","\n","    # Channelwise mean&var\n","    n_channel += seq_len\n","    delta_channel = x - mean_channel\n","    mean_channel = mean_channel + delta_channel.sum(axis=0)/n_channel\n","    m2_channel = m2_channel + (delta_channel*(x - mean_channel)).sum(axis=0)\n","\n","    # Global min&max values.\n","    min_all = np.min(x) if np.min(x) < min_all else min_all\n","    max_all = np.max(x) if np.max(x) > max_all else max_all\n","    \n","    # Min&max sequence length.\n","    min_seq_len = seq_len if seq_len < min_seq_len else min_seq_len\n","    max_seq_len = seq_len if seq_len > max_seq_len else max_seq_len\n","\n","  var_all = m2_all/(n_all-1)\n","  var_channel = m2_channel/(n_channel-1)\n","  stats = dict(mean_all=mean_all, var_all=var_all, mean_channel=mean_channel, var_channel=var_channel, min_all=min_all, max_all=max_all, min_seq_len=min_seq_len, max_seq_len=max_seq_len, n_samples=n_samples)\n","  return stats\n","\n","training_dataset = tf.data.Dataset.from_tensor_slices({\"dummy_samples\":np.float32(samples), \"labels\": samples})\n","start_time = time.time()\n","stats = tf_online_statistics_sequences(training_dataset, key=\"dummy_samples\")\n","print(\"--- %s seconds ---\" % (time.time() - start_time))\n","\n","print(\"[TF Online] mean: {0}, std: {1}\".format(stats[\"mean_all\"], stats[\"var_all\"]))\n","print(\"[TF Online] mean channel: {0}, std channel: {1}\".format(stats[\"mean_channel\"], stats[\"var_channel\"]))\n","print(\"[TF Online] # samples: {0}\".format(stats[\"n_samples\"]))\n","print(\"[TF Online] min value: {0}, max value: {1}\".format(stats[\"min_all\"], stats[\"max_all\"]))\n","print(\"[TF Online] min length: {0}, max length: {1}\".format(stats[\"min_seq_len\"], stats[\"max_seq_len\"]))\n","print(\"============\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wU_aU-Wjootk","colab_type":"code","colab":{}},"cell_type":"code","source":["def online_statistics_sequences(data):\n","  n_samples = 0\n","  n, mean, var, m2 = 0, 0, 0, 0\n","\n","  for x in data: \n","    n_samples += 1\n","\n","    # Global mean&var\n","    n += 1\n","    delta = x - mean\n","    mean = mean + delta/n\n","    m2 = m2 + delta*(x - mean)\n","\n","  var = m2/(n-1)\n","  return mean, var"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SFw08Y0i34Mo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"outputId":"cf1c15d7-84fb-47c6-ac14-f5ef2f5993c4","executionInfo":{"status":"ok","timestamp":1550246290521,"user_tz":-60,"elapsed":492,"user":{"displayName":"Emre Aksan","photoUrl":"","userId":"07313602013607958558"}}},"cell_type":"code","source":["mean, var = online_statistics_sequences([10, 2, 3, 9])\n","print(\"[Online] mean: {0}, std: {1}\".format(mean, var))\n","\n","mean, var = online_statistics_sequences([10, 2, 9, 3])\n","print(\"[Online] mean: {0}, std: {1}\".format(mean, var))\n","\n","data = np.array([10, 2, 3, 9])\n","mean = data.mean()\n","var = data.var(ddof=1)\n","print(\"[Offline] mean: {0}, std: {1}\".format(mean, var))"],"execution_count":17,"outputs":[{"output_type":"stream","text":["[Online] mean: 6.0, std: 16.666666666666668\n","[Online] mean: 6.0, std: 16.666666666666668\n","[Offline] mean: 6.0, std: 16.666666666666668\n"],"name":"stdout"}]},{"metadata":{"id":"R85ZdXI7K2uN","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}