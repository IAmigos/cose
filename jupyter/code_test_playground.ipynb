{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"code_test_playground.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python2","display_name":"Python 2"}},"cells":[{"metadata":{"id":"iCYgqHHud0Fv","colab_type":"code","colab":{}},"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","\n","tf.enable_eager_execution()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7D5GWScyeGiR","colab_type":"code","colab":{}},"cell_type":"code","source":["class Constants(object):\n","    # RNN cells and layer types.\n","    LSTM = 'lstm'\n","    GRU = 'gru'\n","    DENSE = \"dense\"  # Fully connected layer.\n","    TCN = \"tcn\"  # Temporal convolutional layer, i.e., causal 1D convolution.\n","    \n","    # Activation functions.\n","    RELU = 'relu'\n","    ELU = 'elu'\n","    SIGMOID = 'sigmoid'\n","    SOFTPLUS = 'softplus'\n","    TANH = 'tanh'\n","    SOFTMAX = 'softmax'\n","    LRELU = 'lrelu'\n","    CLRELU = 'clrelu'  # Clamped leaky relu."],"execution_count":0,"outputs":[]},{"metadata":{"id":"xX-cEOhteZqa","colab_type":"code","colab":{}},"cell_type":"code","source":["C = Constants()\n","tf_input_step = tf.Variable(np.random.normal(0,1, (10, 2)), dtype=tf.float32)\n","tf_input_seq = tf.Variable(np.random.normal(0,1, (10, 20, 2)), dtype=tf.float32)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Sr227UKfd2-t","colab_type":"code","colab":{}},"cell_type":"code","source":["class Activations(object):\n","    @classmethod\n","    def get(cls, type_str):\n","        # Check if the activation is already callable.\n","        if callable(type_str):\n","            return type_str\n","\n","        # Check if the activation is a built-in or custom function.\n","        if type_str == C.RELU:\n","            return tf.nn.relu\n","        elif type_str == C.ELU:\n","            return tf.nn.elu\n","        elif type_str == C.TANH:\n","            return tf.nn.tanh\n","        elif type_str == C.SIGMOID:\n","            return tf.nn.sigmoid\n","        elif type_str == C.SOFTPLUS:\n","            return tf.nn.softplus\n","        elif type_str == C.SOFTMAX:\n","            return tf.nn.softmax\n","        elif type_str == C.LRELU:\n","            return lambda x: tf.nn.leaky_relu(x, alpha=1. / 3.)\n","        elif type_str == C.CLRELU:\n","            with tf.name_scope('ClampedLeakyRelu'):\n","                return lambda x: tf.clip_by_value(tf.nn.leaky_relu(x, alpha=1. / 3.), -3.0, 3.0)\n","        elif type_str is None:\n","            return None\n","        else:\n","            err_unknown_type(type_str)\n","            \n","\n","class RNNCells(object):\n","    @classmethod\n","    def get(cls, type_str, units, layers=1, **kwargs):\n","        cells = []\n","\n","        for i in range(layers):\n","            if type_str == C.LSTM:\n","                cells.append(tf.keras.layers.LSTMCell(units))\n","            elif type_str == C.GRU:\n","                cells.append(tf.keras.layers.GRUCell(units))\n","            else:\n","                err_unknown_type(type_str)\n","\n","        if layers > 1:\n","            return tf.keras.layers.StackedRNNCells(cells)\n","        else:\n","            return cells[0]\n","          \n","          \n","class DenseLayer(tf.keras.models.Sequential):\n","    \"\"\"\n","    Stacks a number of dense layers by allowing applying dropout on the inputs and activation function on the outputs\n","    of every dense layer.\n","    \"\"\"\n","    def __init__(self, units, layers, activation, dropout_rate=0, **kwargs):\n","        super(DenseLayer, self).__init__(**kwargs)\n","\n","        self.num_units = units if isinstance(units, list) else [units] * layers\n","        self.num_layers = layers\n","        self.dropout_rate = dropout_rate if isinstance(dropout_rate, list) else [dropout_rate] * self.num_layers\n","        self.activation_fn = Activations.get(activation)\n","\n","        for idx in range(self.num_layers):\n","            if self.dropout_rate[idx] > 0:\n","                self.add(tf.keras.layers.Dropout(self.dropout_rate[idx], name=self.name + \"_dropout\" + str(idx)))\n","            self.add(tf.keras.layers.Dense(self.num_units[idx], self.activation_fn, name=self.name + \"_\" + str(idx)))\n","\n","    def call(self, inputs, training=None, mask=None):\n","        out = super(DenseLayer, self).call(inputs, training=training, mask=mask)\n","        return out\n","\n","    def get_config(self):\n","        base_config = super(DenseLayer, self).get_config()\n","        return base_config\n","      \n","      \n","class RNNSeq2Seq(tf.keras.models.Sequential):\n","    \"\"\"\n","    A sequence to sequence model for temporal data. The encoder and decoder networks are RNN instances.\n","    \"\"\"\n","    def __init__(self, latent_units, cell_units, cell_layers, cell_type, activation, bidirectional_encoder=False,\n","                 return_logits=True, **kwargs):\n","        \"\"\"\n","        Args:\n","            latent_units: latent representation dimensionality.\n","            cell_units: encoder/decoder rnn cell/output size.\n","            cell_layers: number of encoder/decoder rnn cells.\n","            cell_type (str): 'lstm' or 'gru'.\n","            activation: activation function to be applied after dense layers.\n","            bidirectional_encoder (bool):\n","            return_logits (bool): if True, the output is the same size with encoder inputs. Otherwise, it is decoder\n","                cell's output. It is useful for stacking layers or making arbitrary predictions.\n","            **kwargs:\n","        \"\"\"\n","        super(RNNSeq2Seq, self).__init__(**kwargs)\n","\n","        self.num_latent_units = latent_units\n","        self.num_cell_units = cell_units\n","        self.num_cell_layers = cell_layers\n","        self.cell_type = cell_type\n","        self.activation_fn = Activations.get(activation)\n","        self.is_bidirectional_encoder = bidirectional_encoder\n","        self.return_logits = return_logits\n","\n","        # Encoder and decoder cells.\n","        self.encoder_cell = RNNCells.get(self.cell_type, self.num_cell_units, self.num_cell_layers)\n","        self.decoder_cell = RNNCells.get(self.cell_type, self.num_cell_units, self.num_cell_layers)\n","\n","        # Encoder network.\n","        self.encoder = tf.keras.Sequential()\n","        self.encoder.add(tf.keras.layers.Dense(self.num_cell_units, activation=self.activation_fn))\n","        if self.is_bidirectional_encoder:\n","            self.encoder.add(tf.keras.layers.Bidirectional(tf.keras.layers.RNN(self.encoder_cell)))\n","        else:\n","            self.encoder.add(tf.keras.layers.RNN(self.encoder_cell))\n","        self.encoder.add(tf.keras.layers.Dense(latent_units, activation=None))\n","\n","        # Decoder network.\n","        self.decoder = tf.keras.Sequential()\n","        self.decoder.add(tf.keras.layers.Dense(self.num_cell_units, activation=self.activation_fn))\n","        self.decoder.add(tf.keras.layers.RNN(self.decoder_cell, return_sequences=True))\n","\n","    def build(self, input_shape=None):\n","        # Decoder output is in the same size with the encoder inputs.\n","        if not self.return_logits:\n","            output_units = input_shape[-1]\n","            self.decoder.add(tf.keras.layers.Dense(output_units, activation=None))\n","\n","    def call(self, inputs, decoder_inputs=None, output_len=None, training=None, mask=None, **kwargs):\n","        \"\"\"\n","        Given an input sequence, calculates the embedding and predicts a sequence. If decoder_inputs is passed, then\n","        the decoder is fed with the same input embedding and corresponding decoder_inputs step. If decoder_inputs is\n","        None and output_len passed, then the decoder is fed with its own predictions at the next step.\n","        The length of the output sequence is determined by either the length of the decoder_inputs or or output_len.\n","\n","        Args:\n","            inputs: [batch_size, seq_len, feature_size]\n","            decoder_inputs: [batch_size, seq_len, feature_size]\n","            output_len (int): length of output sequence.\n","            **kwargs:\n","        Returns:\n","            [batch_size, seq_len, num_cell_units] if return_logits is True else [batch_size, seq_len, feature_size]\n","        \"\"\"\n","        assert decoder_inputs is not None or output_len is not None, \"One of the decoder_inputs or output_len must be provided.\"\n","\n","        embedding = self.encoder(inputs)  # [batch_size, latent_size]\n","        if decoder_inputs is not None:\n","            output_len = decoder_inputs.shape[1]\n","            embedding_seq = tf.tile(tf.expand_dims(embedding, axis=1), (1, output_len, 1))\n","            emb_dec_input = tf.concat([embedding_seq, decoder_inputs], axis=-1)\n","            return self.decoder(emb_dec_input)\n","        else:\n","            err_not_implemented(\"autoregressive decoder.\")\n","\n","    def autoregressive_call(self, embedding, num_steps, initial_step=None):\n","        pass\n","\n","    def get_config(self):\n","        base_config = super(RNNSeq2Seq, self).get_config()\n","        return base_config"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uU7ZWypYd-LJ","colab_type":"code","colab":{}},"cell_type":"code","source":["cell = RNNCells.get(\"lstm\", 64, 1)\n","rnn_model = tf.keras.layers.RNN(cell)\n","\n","dense_model = DenseLayer(128, 2, None, name=None)\n","\n","seq2seq_model = RNNSeq2Seq(13, 64, 1, C.LSTM, C.RELU)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FRp2k3bVeNj5","colab_type":"code","colab":{}},"cell_type":"code","source":["seq_out = rnn_model(tf_input_seq)\n","dense_out = dense_model(tf_input_step)\n","seq2seq_out = seq2seq_model(tf_input_seq, decoder_inputs=tf_input_seq)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ckNAlr9ae3EI","colab_type":"code","colab":{"height":305},"outputId":"379249ba-e303-4c00-8da5-e5504aa180d4","executionInfo":{"status":"ok","timestamp":1548433677560,"user_tz":-60,"elapsed":323,"user":{"displayName":"Emre Aksan","photoUrl":"","userId":"07313602013607958558"}}},"cell_type":"code","source":["print(seq2seq_out.shape)\n","seq2seq_model.summary()"],"execution_count":18,"outputs":[{"output_type":"stream","text":["(10, 20, 64)\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","lstm_cell_10 (LSTMCell)      multiple                  33024     \n","_________________________________________________________________\n","lstm_cell_11 (LSTMCell)      multiple                  33024     \n","_________________________________________________________________\n","sequential_6 (Sequential)    multiple                  34061     \n","_________________________________________________________________\n","sequential_7 (Sequential)    multiple                  34048     \n","=================================================================\n","Total params: 68,109\n","Trainable params: 68,109\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"id":"miu0oUS4fNrZ","colab_type":"code","colab":{"height":215},"outputId":"ebaea90f-5e7a-4e2f-8110-7d59a1e2b666","executionInfo":{"status":"ok","timestamp":1548433063587,"user_tz":-60,"elapsed":315,"user":{"displayName":"Emre Aksan","photoUrl":"","userId":"07313602013607958558"}}},"cell_type":"code","source":["dense_model.summary()"],"execution_count":8,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_layer_0 (Dense)        multiple                  384       \n","_________________________________________________________________\n","dense_layer_1 (Dense)        multiple                  16512     \n","=================================================================\n","Total params: 16,896\n","Trainable params: 16,896\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"id":"0XLKbafZke2J","colab_type":"code","colab":{"height":35},"outputId":"9554e3b9-e578-424f-d807-aa514f10cb84","executionInfo":{"status":"ok","timestamp":1548433219902,"user_tz":-60,"elapsed":676,"user":{"displayName":"Emre Aksan","photoUrl":"","userId":"07313602013607958558"}}},"cell_type":"code","source":["dense_model.outputs"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[]"]},"metadata":{"tags":[]},"execution_count":20}]},{"metadata":{"id":"_-tgQx0SyXj7","colab_type":"code","colab":{"height":35},"outputId":"c4771956-8bb5-40cb-a2cc-1e4a37d3fc27","executionInfo":{"status":"ok","timestamp":1548433102462,"user_tz":-60,"elapsed":418,"user":{"displayName":"Emre Aksan","photoUrl":"","userId":"07313602013607958558"}}},"cell_type":"code","source":["tf.shape(dense_out)[1].numpy()"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["128"]},"metadata":{"tags":[]},"execution_count":15}]},{"metadata":{"id":"FiWhzntNzJbs","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}